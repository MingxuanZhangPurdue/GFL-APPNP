{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46bd914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import *\n",
    "from utils.train_helpers import *\n",
    "from models.models import *\n",
    "from models.setup import *\n",
    "import networkx as nx\n",
    "import pickle\n",
    " \n",
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53e2096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_seeds = [101993, 124709, 196252,  95930,  68222, 101539,  22989,  45367,\n",
    "       166831, 189085,  12237, 242044,  40182,  84405, 234468, 233451,\n",
    "       154898,  81745,  70716,  39777, 248183, 109371, 112311, 229323,\n",
    "         2160, 219137, 221729,  98972, 238056, 265088,  90081, 271232,\n",
    "       260735,  96076, 121375,  11447]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e48900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cc_ids = [1, 0, 2, 3, 2, 1, 2, 1, 2, 2, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 3, 0,\n",
    "       0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 3, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abdcc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subcora(rs, ith_cc, G, cora_data, Xs, ys, A, subgraph_size = 300):\n",
    "    torch.manual_seed(rs)\n",
    "    ids = torch.randperm(cora_data.y.shape[0]).numpy()\n",
    "    sub_ids = ids[0:800]\n",
    "    subcora = G.subgraph(ids[0:800])\n",
    "    connected_components = sorted(nx.connected_components(subcora), key=len, reverse=True)\n",
    "    train_nodes = np.array(list(connected_components[ith_cc]))\n",
    "    all_reachable_nodes = []\n",
    "    for train_node in train_nodes:\n",
    "        for reachable_node in nx.bfs_tree(G,source=train_node, depth_limit=2):\n",
    "            all_reachable_nodes.append(reachable_node)\n",
    "    avail_nodes = np.array(list(set(all_reachable_nodes) - set(train_nodes)))\n",
    "    num_train = len(train_nodes)\n",
    "    num_val = num_train\n",
    "    val_nodes = avail_nodes[:num_train]\n",
    "    test_nodes = avail_nodes[num_train:subgraph_size-num_train]\n",
    "    all_ids = np.concatenate([train_nodes,val_nodes,test_nodes], axis=0)\n",
    "    \n",
    "    sub_Xs = Xs[all_ids]\n",
    "    sub_ys = ys[all_ids]\n",
    "    sub_A = A[all_ids,][:,all_ids]\n",
    "    \n",
    "    train_mask = np.arange(num_train)\n",
    "    val_mask = np.arange(num_train, num_train+num_val)\n",
    "    test_mask = np.arange(num_train+num_val,subgraph_size)\n",
    "    \n",
    "    X = sub_Xs.view(subgraph_size, -1)\n",
    "    y = sub_ys.view(subgraph_size)\n",
    "\n",
    "    edge_index = []\n",
    "    N = sub_A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if (i != j):\n",
    "                if (sub_A[i,j] == 1):\n",
    "                    edge_index.append([i,j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    subcora_data = Data(x=X, y=y, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    \n",
    "    return subcora_data, sub_Xs, sub_ys, sub_A, calculate_Atilde(sub_A, 10, 0.9), train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7892201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_exp(method, train_ids, val_ids, test_ids,\n",
    "           ith, folder_path, Xs, ys, A_tilde,\n",
    "           num_communication=500, batch_size=1,\n",
    "           learning_rate=0.01, I=10,\n",
    "           gradient=True, \n",
    "           gradient_noise=True, hidden_noise=True,\n",
    "           gn_std=0.01, hn_std=0.01,\n",
    "           Print=True, print_time=5,\n",
    "           resume=False,\n",
    "           bias=False,\n",
    "           save=True):\n",
    "    \n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    init_mlp = MLP(Xs[0].shape[1], 64, 7, bias)\n",
    "    \n",
    "    server = set_up_NC(Xs, ys, init_mlp, A_tilde, \n",
    "                       train_ids, val_ids, test_ids,\n",
    "                       gradient,\n",
    "                       hidden_noise, gradient_noise,\n",
    "                       hn_std, gn_std)\n",
    "        \n",
    "    grad=\"noisy_grad/\"\n",
    "        \n",
    "    if resume:\n",
    "        checkpoint = torch.load(folder_path + m + grad + \"I\" + str(I) + \"/model_\" + str(ith))\n",
    "        tl = np.load(folder_path + m + grad + \"I\" + str(I) + \"/tl_\" + str(ith)+\".npy\")\n",
    "        ta = np.load(folder_path + m + grad + \"I\" + str(I) + \"/ta_\" + str(ith)+\".npy\")\n",
    "        vl = np.load(folder_path + m + grad + \"I\" + str(I) + \"/vl_\" + str(ith)+\".npy\")\n",
    "        va = np.load(folder_path + m + grad + \"I\" + str(I) + \"/va_\" + str(ith)+\".npy\")\n",
    "    \n",
    "        tl, ta, vl, va = train_NC(server, num_communication, batch_size, learning_rate, I,\n",
    "                                  gradient, noise, \n",
    "                                  Print, print_time,\n",
    "                                  checkpoint, tl, ta, vl, va)\n",
    "        \n",
    "    else:\n",
    "        tl, ta, vl, va = train_NC(server, num_communication, batch_size, learning_rate, I, Print, print_time=10)\n",
    "    if save:    \n",
    "        np.save(folder_path + m + grad + \"I\" + str(I) + \"/tl_\" + str(ith), tl)\n",
    "        np.save(folder_path + m + grad + \"I\" + str(I) + \"/ta_\" + str(ith), ta)\n",
    "        np.save(folder_path + m + grad + \"I\" + str(I) + \"/vl_\" + str(ith), vl)\n",
    "        np.save(folder_path + m + grad + \"I\" + str(I) + \"/va_\" + str(ith), va)\n",
    "\n",
    "        PATH = folder_path + m + grad + \"I\" + str(I) + \"/model_\" + str(ith)\n",
    "        torch.save({\n",
    "                'best_model_state_dict': server.best_cmodel.state_dict(),\n",
    "                'learning_rate': learning_rate,\n",
    "                'test_acc': server.eval_test()[1],\n",
    "                'model_state_dict': server.cmodel.state_dict(),\n",
    "                'best_valloss': server.best_valloss,\n",
    "                'best_valacc': server.best_valacc,\n",
    "                }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c83b751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "cora_data = dataset[0]\n",
    "\n",
    "G, Xs, ys, A = pygdata_to_frameformat(cora_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab104bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 10 Average train loss: 1.87935 Average train accuracy: 0.676 Average val loss: 1.93716 Average val accuracy: 0.206\n",
      "Communication: 20 Average train loss: 1.78352 Average train accuracy: 0.794 Average val loss: 1.91616 Average val accuracy: 0.353\n",
      "Communication: 30 Average train loss: 1.63577 Average train accuracy: 0.794 Average val loss: 1.87629 Average val accuracy: 0.382\n",
      "Communication: 40 Average train loss: 1.44261 Average train accuracy: 0.824 Average val loss: 1.82358 Average val accuracy: 0.441\n",
      "Communication: 50 Average train loss: 1.23765 Average train accuracy: 0.824 Average val loss: 1.77553 Average val accuracy: 0.471\n",
      "Communication: 60 Average train loss: 1.04336 Average train accuracy: 0.882 Average val loss: 1.74090 Average val accuracy: 0.471\n",
      "Communication: 70 Average train loss: 0.86983 Average train accuracy: 0.912 Average val loss: 1.71738 Average val accuracy: 0.471\n",
      "Communication: 80 Average train loss: 0.72248 Average train accuracy: 0.912 Average val loss: 1.70089 Average val accuracy: 0.471\n",
      "Communication: 90 Average train loss: 0.60306 Average train accuracy: 0.941 Average val loss: 1.68922 Average val accuracy: 0.471\n",
      "Communication: 100 Average train loss: 0.50781 Average train accuracy: 0.941 Average val loss: 1.68072 Average val accuracy: 0.412\n",
      "Communication: 110 Average train loss: 0.43249 Average train accuracy: 0.971 Average val loss: 1.67431 Average val accuracy: 0.441\n",
      "Communication: 120 Average train loss: 0.37215 Average train accuracy: 1.000 Average val loss: 1.66943 Average val accuracy: 0.412\n",
      "Communication: 130 Average train loss: 0.32317 Average train accuracy: 1.000 Average val loss: 1.66578 Average val accuracy: 0.412\n",
      "Communication: 140 Average train loss: 0.28255 Average train accuracy: 1.000 Average val loss: 1.66297 Average val accuracy: 0.412\n",
      "Communication: 150 Average train loss: 0.24851 Average train accuracy: 1.000 Average val loss: 1.66094 Average val accuracy: 0.412\n",
      "Communication: 160 Average train loss: 0.21945 Average train accuracy: 1.000 Average val loss: 1.65967 Average val accuracy: 0.441\n",
      "Communication: 170 Average train loss: 0.19459 Average train accuracy: 1.000 Average val loss: 1.65884 Average val accuracy: 0.441\n",
      "Communication: 180 Average train loss: 0.17300 Average train accuracy: 1.000 Average val loss: 1.65839 Average val accuracy: 0.441\n",
      "Communication: 190 Average train loss: 0.15430 Average train accuracy: 1.000 Average val loss: 1.65813 Average val accuracy: 0.412\n",
      "Communication: 200 Average train loss: 0.13805 Average train accuracy: 1.000 Average val loss: 1.65835 Average val accuracy: 0.353\n",
      "Communication: 210 Average train loss: 0.12389 Average train accuracy: 1.000 Average val loss: 1.65886 Average val accuracy: 0.353\n",
      "Communication: 220 Average train loss: 0.11154 Average train accuracy: 1.000 Average val loss: 1.65929 Average val accuracy: 0.353\n",
      "Communication: 230 Average train loss: 0.10081 Average train accuracy: 1.000 Average val loss: 1.66015 Average val accuracy: 0.382\n",
      "Communication: 240 Average train loss: 0.09142 Average train accuracy: 1.000 Average val loss: 1.66106 Average val accuracy: 0.382\n",
      "Communication: 250 Average train loss: 0.08315 Average train accuracy: 1.000 Average val loss: 1.66220 Average val accuracy: 0.382\n",
      "Communication: 260 Average train loss: 0.07596 Average train accuracy: 1.000 Average val loss: 1.66336 Average val accuracy: 0.382\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"experiments/cora/result/\"\n",
    "m = \"GFLAPPNP/\"\n",
    "grad = \"biased_grad/\"\n",
    "I = 10\n",
    "for ith in range(20):\n",
    "    subcora_data, sub_Xs, sub_ys, sub_A, sub_Atilde, train_ids, val_ids, test_ids = generate_subcora(final_seeds[ith],\n",
    "                                                                                                     final_cc_ids[ith],\n",
    "                                                                                                     G,\n",
    "                                                                                                     cora_data, \n",
    "                                                                                                     Xs, ys, \n",
    "                                                                                                     A,\n",
    "                                                                                                     subgraph_size=300)\n",
    "    torch.manual_seed(0)\n",
    "    init_mlp = MLP(sub_Xs[0].shape[1], 64, 7, bias=False)\n",
    "    \n",
    "    rs_exp(m, train_ids, val_ids, test_ids,\n",
    "           ith, folder_path, sub_Xs, sub_ys, sub_Atilde,\n",
    "           num_communication=500, batch_size=1,\n",
    "           learning_rate=0.01, I=10,\n",
    "           Print=True, print_time=10,\n",
    "           resume=False,\n",
    "           bias=False,\n",
    "           save=False)\n",
    "    print (ith, \"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
