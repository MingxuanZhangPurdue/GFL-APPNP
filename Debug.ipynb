{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dff27b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.generate_csbm import *\n",
    "from utils.utils import *\n",
    "from utils.train_helpers import train_NC, train_APPNP, train_pyg_model\n",
    "from utils.utils import calculate_Atilde\n",
    "from models.models import MLP\n",
    "from models.setup import set_up_NC\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f863dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_seeds = [101993, 124709, 196252,  95930,  68222, 101539,  22989,  45367,\n",
    "       166831, 189085,  12237, 242044,  40182,  84405, 234468, 233451,\n",
    "       154898,  81745,  70716,  39777, 248183, 109371, 112311, 229323,\n",
    "         2160, 219137, 221729,  98972, 238056, 265088,  90081, 271232,\n",
    "       260735,  96076, 121375,  11447]\n",
    "final_cc_ids = [1, 0, 2, 3, 2, 1, 2, 1, 2, 2, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 3, 0,\n",
    "       0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 3, 0, 0, 1]\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "cora_data = dataset[0]\n",
    "G, Xs, ys, A = pygdata_to_frameformat(cora_data)\n",
    "\n",
    "def generate_subcora(rs, ith_cc, \n",
    "                     G, cora_data, Xs, ys, A, \n",
    "                     subgraph_size = 300):\n",
    "    torch.manual_seed(rs)\n",
    "    ids = torch.randperm(cora_data.y.shape[0]).numpy()\n",
    "    sub_ids = ids[0:800]\n",
    "    subcora = G.subgraph(ids[0:800])\n",
    "    connected_components = sorted(nx.connected_components(subcora), key=len, reverse=True)\n",
    "    train_nodes = np.array(list(connected_components[ith_cc]))\n",
    "    all_reachable_nodes = []\n",
    "    for train_node in train_nodes:\n",
    "        for reachable_node in nx.bfs_tree(G,source=train_node, depth_limit=2):\n",
    "            all_reachable_nodes.append(reachable_node)\n",
    "    avail_nodes = np.array(list(set(all_reachable_nodes) - set(train_nodes)))\n",
    "    num_train = len(train_nodes)\n",
    "    num_val = num_train\n",
    "    val_nodes = avail_nodes[:num_train]\n",
    "    test_nodes = avail_nodes[num_train:subgraph_size-num_train]\n",
    "    all_ids = np.concatenate([train_nodes,val_nodes,test_nodes], axis=0)\n",
    "    \n",
    "    sub_Xs = Xs[all_ids]\n",
    "    sub_ys = ys[all_ids]\n",
    "    sub_A = A[all_ids,][:,all_ids]\n",
    "    \n",
    "    train_mask = np.arange(num_train)\n",
    "    val_mask = np.arange(num_train, num_train+num_val)\n",
    "    test_mask = np.arange(num_train+num_val,subgraph_size)\n",
    "    \n",
    "    X = sub_Xs.view(subgraph_size, -1)\n",
    "    y = sub_ys.view(subgraph_size)\n",
    "\n",
    "    edge_index = []\n",
    "    N = sub_A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if (i != j):\n",
    "                if (sub_A[i,j] == 1):\n",
    "                    edge_index.append([i,j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    subcora_data = Data(x=X, y=y, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    \n",
    "    return subcora_data, sub_Xs, sub_ys, sub_A, calculate_Atilde(sub_A, 10, 0.9), train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5e6fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_idx=0\n",
    "subcora_data, sub_Xs, sub_ys, sub_A, sub_Atilde, train_ids, val_ids, test_ids = generate_subcora(final_seeds[seed_idx],\n",
    "                                                                                                 final_cc_ids[seed_idx],\n",
    "                                                                                                 G,\n",
    "                                                                                                 cora_data, \n",
    "                                                                                                 Xs, ys, \n",
    "                                                                                                 A,\n",
    "                                                                                                 subgraph_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de0bb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, APPNP\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx, from_scipy_sparse_matrix\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "class appnp(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, hidden_dimensions=64, bias=False,\n",
    "                 K=10, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.appnp_layer = APPNP(K=K, alpha=alpha, cached=True)\n",
    "        self.linear = nn.Linear(num_node_features, hidden_dimensions, bias=bias)\n",
    "        self.output = nn.Linear(hidden_dimensions, num_classes, bias=bias)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(self.appnp_layer(x, edge_index))\n",
    "        return F.log_softmax(x, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e02b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = appnp(1433, 7, bias=True).to(device)\n",
    "data = subcora_data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "tl = []\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    tl.append(loss.item())\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1bda2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6034\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.shape[0])\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0eb2001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x208c958f670>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e56e3ac2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [125]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m from_scipy_sparse_matrix(sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(A))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39m\u001b[43mX\u001b[49m, y\u001b[38;5;241m=\u001b[39my, edge_index\u001b[38;5;241m=\u001b[39medge_index)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "edge_index = from_scipy_sparse_matrix(sparse.csr_matrix(A))[0]\n",
    "data = Data(x=X, y=y, edge_index=edge_index, train_mask=train_ids, valid_mask=valid_mask, test_mask=test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ffe259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\datascience\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Mingxuan Zhang\\Desktop\\GFL-APPNP\\utils\\utils.py:57: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = np.array(adjacency_matrix(G).todense())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication: 5 Average train loss: 1.91644 Average train accuracy: 0.412 Average val loss: 1.94300 Average val accuracy: 0.206\n",
      "Communication: 10 Average train loss: 1.88347 Average train accuracy: 0.618 Average val loss: 1.93731 Average val accuracy: 0.206\n",
      "Communication: 15 Average train loss: 1.84249 Average train accuracy: 0.794 Average val loss: 1.92880 Average val accuracy: 0.294\n",
      "Communication: 20 Average train loss: 1.78959 Average train accuracy: 0.824 Average val loss: 1.91626 Average val accuracy: 0.353\n",
      "Communication: 25 Average train loss: 1.72376 Average train accuracy: 0.794 Average val loss: 1.89869 Average val accuracy: 0.412\n",
      "Communication: 30 Average train loss: 1.64444 Average train accuracy: 0.794 Average val loss: 1.87648 Average val accuracy: 0.382\n",
      "Communication: 35 Average train loss: 1.55270 Average train accuracy: 0.794 Average val loss: 1.85101 Average val accuracy: 0.382\n",
      "Communication: 40 Average train loss: 1.45286 Average train accuracy: 0.824 Average val loss: 1.82391 Average val accuracy: 0.441\n",
      "Communication: 45 Average train loss: 1.35011 Average train accuracy: 0.824 Average val loss: 1.79840 Average val accuracy: 0.471\n",
      "Communication: 50 Average train loss: 1.24778 Average train accuracy: 0.824 Average val loss: 1.77591 Average val accuracy: 0.471\n",
      "Communication: 55 Average train loss: 1.14802 Average train accuracy: 0.882 Average val loss: 1.75694 Average val accuracy: 0.500\n",
      "Communication: 60 Average train loss: 1.05234 Average train accuracy: 0.882 Average val loss: 1.74135 Average val accuracy: 0.471\n",
      "Communication: 65 Average train loss: 0.96176 Average train accuracy: 0.912 Average val loss: 1.72844 Average val accuracy: 0.471\n",
      "Communication: 70 Average train loss: 0.87715 Average train accuracy: 0.912 Average val loss: 1.71788 Average val accuracy: 0.471\n",
      "Communication: 75 Average train loss: 0.79921 Average train accuracy: 0.912 Average val loss: 1.70893 Average val accuracy: 0.471\n",
      "Communication: 80 Average train loss: 0.72830 Average train accuracy: 0.912 Average val loss: 1.70140 Average val accuracy: 0.471\n",
      "Communication: 85 Average train loss: 0.66446 Average train accuracy: 0.941 Average val loss: 1.69505 Average val accuracy: 0.471\n",
      "Communication: 90 Average train loss: 0.60732 Average train accuracy: 0.941 Average val loss: 1.68964 Average val accuracy: 0.471\n",
      "Communication: 95 Average train loss: 0.55634 Average train accuracy: 0.941 Average val loss: 1.68506 Average val accuracy: 0.471\n",
      "Communication: 100 Average train loss: 0.51104 Average train accuracy: 0.941 Average val loss: 1.68111 Average val accuracy: 0.412\n",
      "Communication: 105 Average train loss: 0.47079 Average train accuracy: 0.971 Average val loss: 1.67769 Average val accuracy: 0.412\n",
      "Communication: 110 Average train loss: 0.43491 Average train accuracy: 1.000 Average val loss: 1.67464 Average val accuracy: 0.441\n",
      "Communication: 115 Average train loss: 0.40280 Average train accuracy: 1.000 Average val loss: 1.67200 Average val accuracy: 0.412\n",
      "Communication: 120 Average train loss: 0.37399 Average train accuracy: 1.000 Average val loss: 1.66976 Average val accuracy: 0.412\n",
      "Communication: 125 Average train loss: 0.34805 Average train accuracy: 1.000 Average val loss: 1.66779 Average val accuracy: 0.412\n",
      "Communication: 130 Average train loss: 0.32453 Average train accuracy: 1.000 Average val loss: 1.66603 Average val accuracy: 0.412\n",
      "Communication: 135 Average train loss: 0.30315 Average train accuracy: 1.000 Average val loss: 1.66453 Average val accuracy: 0.412\n",
      "Communication: 140 Average train loss: 0.28364 Average train accuracy: 1.000 Average val loss: 1.66323 Average val accuracy: 0.412\n",
      "Communication: 145 Average train loss: 0.26576 Average train accuracy: 1.000 Average val loss: 1.66213 Average val accuracy: 0.412\n",
      "Communication: 150 Average train loss: 0.24932 Average train accuracy: 1.000 Average val loss: 1.66120 Average val accuracy: 0.412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 99>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m init_mlp \u001b[38;5;241m=\u001b[39m MLP(sub_Xs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m7\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m server \u001b[38;5;241m=\u001b[39m set_up_NC(sub_Xs, sub_ys, init_mlp, sub_Atilde, \n\u001b[0;32m     94\u001b[0m                    train_ids, val_ids, test_ids,\n\u001b[0;32m     95\u001b[0m                    \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m                    \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                    \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m tl, ta, vl, va \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_NC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\GFL-APPNP\\utils\\train_helpers.py:81\u001b[0m, in \u001b[0;36mtrain_NC\u001b[1;34m(server, num_communication, batch_size, learning_rate, I, Print, print_time)\u001b[0m\n\u001b[0;32m     77\u001b[0m train_accs, val_accs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ith \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_communication):\n\u001b[1;32m---> 81\u001b[0m     train_loss, train_acc, val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunication\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     84\u001b[0m     train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "File \u001b[1;32m~\\Desktop\\GFL-APPNP\\models\\GFLAPPNP_NC.py:279\u001b[0m, in \u001b[0;36mCentral_Server.communication\u001b[1;34m(self, batch_size, learning_rate, I)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    278\u001b[0m         C_k \u001b[38;5;241m=\u001b[39m C[k,:] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA_tilde[k,k]\u001b[38;5;241m*\u001b[39mH[k,:]\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA_tilde\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA_tilde_gdevice\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_aggregation_train()\n\u001b[0;32m    283\u001b[0m train_loss, train_acc, val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_train_val()\n",
      "File \u001b[1;32m~\\Desktop\\GFL-APPNP\\models\\GFLAPPNP_NC.py:127\u001b[0m, in \u001b[0;36mNode.local_update\u001b[1;34m(self, A_tilde_k_d, A_tilde_k_gd, C_k, dH, batch_size, learning_rate, I)\u001b[0m\n\u001b[0;32m    125\u001b[0m             p_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi,ibcd->ibcd\u001b[39m\u001b[38;5;124m\"\u001b[39m, A_tilde_k_gd[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_mask], dH[pname][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_mask])\n\u001b[0;32m    126\u001b[0m             p_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab,cbef->ef\u001b[39m\u001b[38;5;124m\"\u001b[39m, Errs, p_grad)\u001b[38;5;241m/\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 127\u001b[0m             param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mp_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     local_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(torch\u001b[38;5;241m.\u001b[39mlog(y_hat), y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.utils import *\n",
    "from utils.train_helpers import *\n",
    "from models.models import *\n",
    "from models.setup import *\n",
    "import networkx as nx\n",
    "import pickle\n",
    " \n",
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from os.path import exists\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "final_seeds = [101993, 124709, 196252,  95930,  68222, 101539,  22989,  45367,\n",
    "       166831, 189085,  12237, 242044,  40182,  84405, 234468, 233451,\n",
    "       154898,  81745,  70716,  39777, 248183, 109371, 112311, 229323,\n",
    "         2160, 219137, 221729,  98972, 238056, 265088,  90081, 271232,\n",
    "       260735,  96076, 121375,  11447]\n",
    "final_cc_ids = [1, 0, 2, 3, 2, 1, 2, 1, 2, 2, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 3, 0,\n",
    "       0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 3, 0, 0, 1]\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "cora_data = dataset[0]\n",
    "G, Xs, ys, A = pygdata_to_frameformat(cora_data)\n",
    "\n",
    "def generate_subcora(rs, ith_cc, \n",
    "                     G, cora_data, Xs, ys, A, \n",
    "                     subgraph_size = 300):\n",
    "    torch.manual_seed(rs)\n",
    "    ids = torch.randperm(cora_data.y.shape[0]).numpy()\n",
    "    sub_ids = ids[0:800]\n",
    "    subcora = G.subgraph(ids[0:800])\n",
    "    connected_components = sorted(nx.connected_components(subcora), key=len, reverse=True)\n",
    "    train_nodes = np.array(list(connected_components[ith_cc]))\n",
    "    all_reachable_nodes = []\n",
    "    for train_node in train_nodes:\n",
    "        for reachable_node in nx.bfs_tree(G,source=train_node, depth_limit=2):\n",
    "            all_reachable_nodes.append(reachable_node)\n",
    "    avail_nodes = np.array(list(set(all_reachable_nodes) - set(train_nodes)))\n",
    "    num_train = len(train_nodes)\n",
    "    num_val = num_train\n",
    "    val_nodes = avail_nodes[:num_train]\n",
    "    test_nodes = avail_nodes[num_train:subgraph_size-num_train]\n",
    "    all_ids = np.concatenate([train_nodes,val_nodes,test_nodes], axis=0)\n",
    "    \n",
    "    sub_Xs = Xs[all_ids]\n",
    "    sub_ys = ys[all_ids]\n",
    "    sub_A = A[all_ids,][:,all_ids]\n",
    "    \n",
    "    train_mask = np.arange(num_train)\n",
    "    val_mask = np.arange(num_train, num_train+num_val)\n",
    "    test_mask = np.arange(num_train+num_val,subgraph_size)\n",
    "    \n",
    "    X = sub_Xs.view(subgraph_size, -1)\n",
    "    y = sub_ys.view(subgraph_size)\n",
    "\n",
    "    edge_index = []\n",
    "    N = sub_A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if (i != j):\n",
    "                if (sub_A[i,j] == 1):\n",
    "                    edge_index.append([i,j])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    subcora_data = Data(x=X, y=y, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    \n",
    "    return subcora_data, sub_Xs, sub_ys, sub_A, calculate_Atilde(sub_A, 10, 0.9), train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subcora_data, sub_Xs, sub_ys, sub_A, sub_Atilde, train_ids, val_ids, test_ids = generate_subcora(final_seeds[0],\n",
    "                                                                                                 final_cc_ids[0],\n",
    "                                                                                                 G,\n",
    "                                                                                                 cora_data, \n",
    "                                                                                                 Xs, ys, \n",
    "                                                                                                 A,\n",
    "                                                                                                 subgraph_size=300)\n",
    "torch.manual_seed(0)\n",
    "init_mlp = MLP(sub_Xs[0].shape[1], 64, 7, bias=False)\n",
    "\n",
    "server = set_up_NC(sub_Xs, sub_ys, init_mlp, sub_Atilde, \n",
    "                   train_ids, val_ids, test_ids,\n",
    "                   True,\n",
    "                   False, False,\n",
    "                   0.01, 0.01)\n",
    "\n",
    "tl, ta, vl, va = train_NC(server, 200, 1, 0.01, 10, True, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
